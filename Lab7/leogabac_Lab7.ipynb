{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT22/blob/leogabac-Lab7/Lab7/leogabac_Lab7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 7: Optimization and learning**\n",
        "**Leonardo Gabriel Alanis Cantú**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfik8rc5mgZg"
      },
      "source": [
        "In this report we look at the gradient descent method in $\\mathbb{R}^n$ for minimization of functions of several variables. The implementations were done in code with acceptable results. Details are explained in the pertinent sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Pdll1Xc9WP0e",
        "outputId": "0dcab556-a1ed-4443-e480-42fb7764270e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2020 Johan Hoffman (jhoffman@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "outputs": [],
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from math import *\n",
        "from numpy import mean\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "The main task of this report is that given a function $f(x_1, \\ldots, x_n)$\n",
        "\n",
        "$$\n",
        "f: \\mathbb{R}^n \\to \\mathbb{R}\n",
        "$$\n",
        "\n",
        "we want to find (iteratively) a local minimum of this function. A way to do this is by the update formula\n",
        "\n",
        "$$\n",
        "x_{k+1} = x_k - \\alpha_k \\hat{n}.\n",
        "$$\n",
        "\n",
        "Here $x$ denotes the vector (x_1,\\ldots,x_n), $\\hat{n}$ denotes the direction in which the following step is going to be made, and $\\alpha_k$ is the size of that step. Since we want to eventually reach a local minimum, a natural choice of $\\hat{n}$ is the negative of the gradient, which intuitively points towards the direction of greatest descend. This method is known as _gradient descent_.\n",
        "\n",
        "$$\n",
        "x_{k+1} = x_k - \\alpha_k \\nabla f(x_k).\n",
        "$$\n",
        "\n",
        "Note that we allow $\\alpha_k$ to be different at each step. A suitable choice of $\\alpha$ is a matter to pause and ponder (a lot), since a small value would slow down the convergence, whereas a big enough value could cause a divergence, hence the value of $\\alpha$ should be bounded between 0 and 1.\n",
        "\n",
        "A \"good\" choice of $\\alpha$ at step $k$ is the one that induces the greatest descent. We can find this value by minimizing\n",
        "\n",
        "$$\n",
        "h(\\alpha_k) = f(x_k - \\alpha_k\\nabla f(x_k)) , \\quad \\alpha_k \\in (0,1)\n",
        "$$\n",
        "\n",
        "which is a another optimization problem, but it is only a line search on the direction of descent.\n",
        "\n",
        "To implement the gradient descent algorithm, we need to be able to compute numerical gradients, hence the necessity of numerical partial derivatives. We will implement this by a centran difference scheme\n",
        "\n",
        "$$\n",
        "\\dfrac{\\partial f}{\\partial x^i}(x) \\approx \\dfrac{f(x + \\tilde{h}) - f(x - \\tilde{h})}{2h}\n",
        "$$\n",
        "\n",
        "where $\\tilde{h}$ is the vector $(0,\\ldots,h,\\ldots,0)^\\text{T}$ with an $h$ in the $i$th position."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zF4iBj5VURZx"
      },
      "source": [
        "Let us implement all needed routines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "G8t74j_ImgZj"
      },
      "outputs": [],
      "source": [
        "def pdv(f,x,variable, h = 0.01):\n",
        "    # ===== INPUT ===== #\n",
        "    # f: Function in several variables\n",
        "    # x: Point at which the derivative is taken at\n",
        "    # variable: Variable to which we are going to differentiate\n",
        "    # h: Spacing\n",
        "    # ===== Output ===== #\n",
        "    # ∂f/∂x[variable] evaluated at x\n",
        "    \n",
        "    h_vec = np.zeros(len(x))\n",
        "    h_vec[variable] = h\n",
        "    \n",
        "    diff = f(x + h_vec) - f(x - h_vec)\n",
        "    \n",
        "    return diff/(2*h)\n",
        "\n",
        "def gradient(f, x):\n",
        "    # ===== INPUT ===== #\n",
        "    # f: Function in several variables\n",
        "    # x: Point at which the gradient is evaluated\n",
        "    # ===== Output ===== #\n",
        "    # ∇f evaluated at x\n",
        "    \n",
        "    n = len(x)\n",
        "    grad = np.array( [pdv(f,x,xi) for xi in range(n)] )\n",
        "    return grad\n",
        "\n",
        "\n",
        "def line_search2(f,x,grad):\n",
        "    \n",
        "    # Extremely sloppy minimization\n",
        "    \n",
        "    a = np.linspace(0,1,50)\n",
        "    feval = [f(x-ai*grad) for ai in a]\n",
        "    ind = np.argmin(feval)\n",
        "    \n",
        "    return a[ind]\n",
        "        \n",
        "    \n",
        "def gradient_descent(f,ansatz, alpha, TOL = 0.01):\n",
        "    # ===== INPUT ===== #\n",
        "    # f: Function to minimize\n",
        "    # ansatz: Initial guess\n",
        "    # alpha: parameter\n",
        "    # ===== Output ===== #\n",
        "    # x s.t. f is minimum\n",
        "    \n",
        "    x_min = np.copy(ansatz) # initialize\n",
        "    grad = np.ones( len(ansatz) ) # initialize\n",
        "    \n",
        "    while np.linalg.norm(grad) > TOL:\n",
        "        grad = gradient(f,x_min)\n",
        "        alpha = line_search2(f,x_min,grad)\n",
        "        x_min = x_min - alpha*grad\n",
        "        \n",
        "    return x_min\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqQSOn8TmgZl"
      },
      "source": [
        "To test our results, we will compare against some functions whose solutions we already know.\n",
        "\n",
        "$$\n",
        "f(x,y) = (x-1)^2 + (y+2)^2 -3\n",
        "$$\n",
        "\n",
        "is a paraboloid centered at $(1,-2)$ (hence its minimum). This test should be simple enough for the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HE9vnCrEmgZl",
        "outputId": "f61873fb-70ee-4f25-cb27-86ce71692829"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 2.854146892646271e-08\n"
          ]
        }
      ],
      "source": [
        "def test_func(x):\n",
        "    return (x[0]-1)**2 + (x[1] + 2)**2 - 3\n",
        "\n",
        "x_min = gradient_descent(test_func, [5,5], 0.1, TOL = 1e-5)\n",
        "diff = np.array([1,-2]) - x_min\n",
        "print(\"Error:\", np.linalg.norm(diff))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FA-XgOWTmgZl"
      },
      "source": [
        "Now, the function\n",
        "\n",
        "$$\n",
        "f(x,y) = \\cos(x) \\sin(y)\n",
        "$$\n",
        "\n",
        "has several minima. In fact, for any two integers $m$ and $n$, the minimum are located at\n",
        "\n",
        "$$\n",
        "x = 2\\pi n, \\quad y = 2\\pi m - \\dfrac{\\pi}{2}\n",
        "$$\n",
        "\n",
        "Let us try to find some of the minima."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhMby8YkmgZl",
        "outputId": "0b333606-b072-46ad-af8c-78981d7c0883"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 9.610881359995597e-11\n"
          ]
        }
      ],
      "source": [
        "def test_func(x):\n",
        "    return cos(x[0])*sin(x[1])\n",
        "\n",
        "x_min = gradient_descent(test_func, [4.125,-1], 0.1, TOL = 1e-5)\n",
        "diff = np.array([2*pi,-pi/2]) - x_min\n",
        "print(\"Error:\", np.linalg.norm(diff))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "# Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS8yoS82mgZm"
      },
      "source": [
        "In this last report we looked at gradient descent. One of the most natural minimization techniques to make whenever we are dealing with functions of many variables. The results were good, which is expected from this type of algorithm.\n",
        "\n",
        "One personal comment, is that I always wanted to implement a numerical gradient. I took the chance to do it here, I was excited."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "leogabac_Lab7.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}