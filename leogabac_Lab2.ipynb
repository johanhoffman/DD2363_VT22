{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT22/blob/leogabac-Lab2/leogabac_Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 2: Matrix Factorization**\n",
        "**Leonardo Gabriel Alanis Cantú**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UFTSzW7P8kL"
      },
      "source": [
        "Matrices are a mathematical object that come to be very useful in many areas of Science. In this report we will explore some topics related to matrices, such as CSR sparse matrices, QR factorization, computation of the inverse to solve systems of linear equations, as well as an approximation method when a system of equations has no solution. The algorithms are explained in the Introduction section, and implemented in Python code in the Methods section. Results were fairly accurate in the different tests that were performed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJipbXtnjrJZ"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkT8J7uOWpT3"
      },
      "source": [
        "#**About the code**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is distributed under a certain license. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Pdll1Xc9WP0e",
        "outputId": "9e782dc7-4ad0-48f9-d45c-2200e83dec4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2020 Johan Hoffman (jhoffman@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "outputs": [],
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from sympy import *\n",
        "import math\n",
        "\n",
        "#try:\n",
        "#    from dolfin import *; from mshr import *\n",
        "#except ImportError as e:\n",
        "#    !apt-get install -y -qq software-properties-common \n",
        "#    !add-apt-repository -y ppa:fenics-packages/fenics\n",
        "#    !apt-get update -qq\n",
        "#    !apt install -y --no-install-recommends fenics\n",
        "#    from dolfin import *; from mshr import *\n",
        "    \n",
        "#import dolfin.common.plotting as fenicsplot\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import tri\n",
        "from matplotlib import axes\n",
        "from mpl_toolkits.mplot3d import Axes3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "Describe the methods you used to solve the problem. This may be a combination of text, mathematical formulas (Latex), algorithms (code), data and output.  \n",
        "\n",
        "Sparse matrices, are matrices for which most of its elements are zeroes. Since anything multiplied by zero is zero, the natural question arises: how can we take advantage of the structure of these matrices to store them in a more compacted way? The _Compressed Sparse Row_ format, stores the values of a matrix in three different lists\n",
        "\n",
        "- ´vals´ contains the non-zero values of $A$.\n",
        "- ´col_idx´ contains the column index of non-zero entries.\n",
        "- ´row_ptr´ contains the non-zero index for which a new row starts.\n",
        "\n",
        "Matrix-vector multiplication is implemented as always, but with a few subtleties regarding the CSR format.\n",
        "\n",
        "A $m\\times n$ matrix can be factorized in the product of two matrices $A = QR$ where $Q$ is orthogonal i.e. $Q Q^{\\text{T}} = I$, and $R$ is an upper triangular matrix. As for the scope of this report, we will consider $A$ a full rank matrix, for which we will form the columms of $Q$ by the Gram-Schmidth process with the columns of $A$. (The details will be taken from the textbook by Hoffman, J.)\n",
        "\n",
        "Given a system of linear equations, it can always be boiled down to the matrix equation $Ax = b$, for which $x$ is unknown. A solver will be implemented in this report by taking advantage that $A$ has a unique solution if it is invertible (which is what we want), or well _full rank_. We reuse the QR factorization algorithm that was implemented, that is\n",
        "\n",
        "$$\n",
        "A^{-1} = \\left(QR\\right)^{-1} = R^{-1}Q^{-1} = R^{-1} Q^{\\text{T}}\n",
        "$$\n",
        "\n",
        "This means that, in order to solve $Ax = b$, we need to multiply $x = R^{-1}Q^{\\text{T}}b$. \n",
        "\n",
        "The matrix $R^{-1}$ seems to be repeating the same problem, but it is not, we can take the advantage that it is by definition an upper triangular, and find its inverse by gaussian reducing it into its unique reduced echelon form.\n",
        "\n",
        "Finally, suppose that $Ax = b$ is an inconsistent system, meaning that there is no solution; the problem arises: which is _the closest_ solution? This is known as a _least squares_ solution. The closest approximation is given by the solution to the new system of equations\n",
        "\n",
        "$$\n",
        "A^{\\text{T}}A x = A^{\\text{T}} b\n",
        "$$\n",
        "\n",
        "However, there is a problem, it is possible that $A^{\\text{T}}A$ is not invertible, meaning that there is an infinite set of solutions, we will not deal with such cases in this report. Since we want $A^{\\text{T}}A$ to be invertible, we want the columns of $A$ to be linearly independent, and that can only happen if $m > n$, hence those are the type of cases that will be taken into consideration.\n",
        "\n",
        "All four algorithms will be implemented in the following section of the report.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfPFpBpX_6i-"
      },
      "source": [
        "CRS Matrix-vector product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RVq55W5_6i-"
      },
      "outputs": [],
      "source": [
        "class sparse_crs:\n",
        "    # Object that contains CSR format information\n",
        "    def __init__(self, vals, colind, rowptr):\n",
        "        self.vals = vals\n",
        "        self.colind = colind\n",
        "        self.rowptr = rowptr\n",
        "\n",
        "# Usually, I wouldn't need this function, but it is for testing purposes\n",
        "def mat2crs(A):\n",
        "    # Input\n",
        "    # A: sparse matrix\n",
        "    \n",
        "    # Output\n",
        "    # CSR data val, col_ind, row_ptr\n",
        "    vals = []\n",
        "    colind = []\n",
        "    rowptr = []\n",
        "    nzind = 0\n",
        "    \n",
        "    for row in range(A.shape[0]):\n",
        "        rowptr.append(nzind)\n",
        "        for col in range(A.shape[1]):\n",
        "            if A[row,col] != 0:\n",
        "                nzind += 1\n",
        "                vals.append( A[row,col] )\n",
        "                colind.append(col)\n",
        "    rowptr.append( rowptr[-1]+1)\n",
        "    \n",
        "    return [vals, colind, rowptr]\n",
        "\n",
        "\n",
        "def crs_matrixvec(A,x):\n",
        "    # Input\n",
        "    # A: sparse CSR object\n",
        "    # x: vector\n",
        "    \n",
        "    # Output\n",
        "    # y = Ax\n",
        "    \n",
        "    n = len(x)\n",
        "    y = np.zeros(n) # initialize\n",
        "    \n",
        "    for i in range(n):\n",
        "        for j in range( A.rowptr[i],A.rowptr[i+1]):\n",
        "            y[i] += A.vals[j]*x[ A.colind[j] ]\n",
        "    \n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDirS4Ma_6i_"
      },
      "source": [
        "QR factorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzUJHdvX_6i_"
      },
      "outputs": [],
      "source": [
        "def QR_factorization_GS(A):\n",
        "    # Input\n",
        "    # A: n x n invertible matrix\n",
        "    \n",
        "    # Output\n",
        "    # Q: Orthogonal n x n matrix\n",
        "    # R: n x n upper triangular matrix\n",
        "    \n",
        "    n = A.shape[0]\n",
        "    Q = np.zeros(A.shape) # initialize\n",
        "    R = np.zeros(A.shape) # initialize\n",
        "    \n",
        "    for j in range(n):\n",
        "        v = A[:,j]\n",
        "        for i in range(j):\n",
        "            R[i,j] = np.dot(Q[:,i],v)\n",
        "            v = v - R[i,j]*Q[:,i]\n",
        "        \n",
        "        R[j,j] = math.sqrt( np.dot(v,v) )\n",
        "        Q[:,j] = v / R[j,j]\n",
        "        \n",
        "    return Q, R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwfQy76V_6i_"
      },
      "source": [
        "Solver of a system of linear equations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11WXK_4i_6i_"
      },
      "outputs": [],
      "source": [
        "def reduceR(R):\n",
        "    # Input\n",
        "    # R: n x n upper triangular invertible matrix\n",
        "    \n",
        "    # Output\n",
        "    # Rinv: inverse matrix of R\n",
        "    \n",
        "    n = R.shape[0]\n",
        "    R = np.concatenate( (R, np.identity(n)) , axis = 1)\n",
        "    for col in reversed(range(n)):\n",
        "        toone = np.identity(n) # initialize\n",
        "        tozero = np.identity(n) # initialize\n",
        "        \n",
        "        toone[col,col] = 1/R[col,col]\n",
        "        tozero[0:col,col] = - R[0:col,col]\n",
        "        E = np.dot(tozero,toone)\n",
        "        R = np.dot(E,R)\n",
        "    return R[:,n:2*n]\n",
        "\n",
        "def solvelinear(A,b):\n",
        "    # Input\n",
        "    # A: n x n invertible matrix\n",
        "    # b: n-vector\n",
        "    \n",
        "    # Output\n",
        "    # x: solution to Ax = b\n",
        "    \n",
        "    Q,R = QR_factorization_GS(A)\n",
        "    Rinv = reduceR(R)\n",
        "    Ainv = np.dot(Rinv,np.transpose(Q))\n",
        "    \n",
        "    return np.dot(Ainv,b)\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGn-OG6F_6jA"
      },
      "source": [
        "Least squares solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmZwn2J5_6jA"
      },
      "outputs": [],
      "source": [
        "def leastsolve(A,b):\n",
        "    # Input\n",
        "    # A: m x n matrix, s.t. m>n\n",
        "    # b: m element vector\n",
        "    \n",
        "    # Output\n",
        "    # x: best approximation of a solution\n",
        "    \n",
        "    newA = np.dot(np.transpose(A),A)\n",
        "    newb = np.dot(np.transpose(A),b)\n",
        "    pivots = Matrix(newA).rref()[1]\n",
        "    assert len(pivots) == newA.shape[1], \"Infinite number of solutions\"\n",
        "    \n",
        "    x = solvelinear(newA,newb) # the one that we programmed earlier\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLwlnOzuV-Cd"
      },
      "source": [
        "For the CSR matrix-vector product, we will test on an arbitrary sparse matrix, and compare it with the usual matrix-vector product from the numpy library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poSEeWi7_6jB",
        "outputId": "6e627514-fefd-418b-e0ed-04f80a37ffe8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CSR: 0.0 seconds\n",
            "Dense: 0.0009970664978027344 seconds\n",
            "|y2-y1| = 0.0\n"
          ]
        }
      ],
      "source": [
        "A = np.array([ [2,0,0,2,0], [3,4,2,5,0], [5,0,0,8,17], [0,0,10,16,0], [0,0,0,0,14] ]) # declare array\n",
        "sparseA = sparse_crs(*mat2crs(A)) # convert to csr\n",
        "x = np.array([1,2,3,4,5]) # \n",
        "\n",
        "start_time = time.time()\n",
        "y1 = crs_matrixvec(sparseA,x)\n",
        "print(\"CSR: %s seconds\" % (time.time() - start_time))\n",
        "\n",
        "start_time = time.time()\n",
        "y2 = np.dot(A,x)\n",
        "print(\"Dense: %s seconds\" % (time.time() - start_time))\n",
        "\n",
        "print(\"|y2-y1| =\", np.linalg.norm(y2-y1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waRx2QDp_6jB"
      },
      "source": [
        "For the QR factorization we will use the Frobenius norm\n",
        "$$\n",
        "\\| A \\|_F = \\sqrt{ \\text{tr} (A A^{\\dagger} ) }\n",
        "$$\n",
        "where $A^\\dagger$ is the adjoint matrix, or the transpose for real matrices. Hence we will compute\n",
        "$$\n",
        "\\| Q Q^{\\text{T}} - I \\|_F,\n",
        "$$\n",
        "and\n",
        "$$\n",
        "\\| Q R - A \\|_F.\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11xBIQSI_6jB",
        "outputId": "c78fafd4-6ff3-4a8a-fe0b-e1c37425e314"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|QQ^T - I| = 3.1836122848239643e-16\n",
            "|QR - A| = 0.0\n"
          ]
        }
      ],
      "source": [
        "def frobenius(A):\n",
        "    return math.sqrt(np.trace(  np.dot( A, np.transpose(A) ) ))\n",
        "\n",
        "A = np.array([ [2,-1],[-1,2] ])\n",
        "Q,R = QR_factorization_GS(A)\n",
        "\n",
        "print(\"|QQ^T - I| =\", frobenius( np.dot(Q, np.transpose(Q)) - np.identity(2)  ))\n",
        "print(\"|QR - A| =\", frobenius( np.dot(Q,R) - A ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLAbnCRT_6jC"
      },
      "source": [
        "To test the solver of linear equations, we will compute the norms $\n",
        "\\| Ax - b\\|$, and $\\| x-y\\|$ where $y$ is the real solution. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBuRvTWA_6jC",
        "outputId": "b31597c3-f5ea-4aaf-e949-4254a549e912"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|Ax - b| = 1.724530548492318e-13\n",
            "|x-y| = 6.316845058634323e-12\n"
          ]
        }
      ],
      "source": [
        "A = np.array([ [1,-2,1],[0,2,-8],[-4,5,9] ])\n",
        "b = np.array([0,8,-9])\n",
        "x = solvelinear(A,b) \n",
        "y = np.array([29,16,3])\n",
        "\n",
        "print(\"|Ax - b| =\", np.linalg.norm(np.dot(A,x) - b ) )\n",
        "print(\"|x-y| =\", np.linalg.norm(x-y) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWR-0USF_6jC"
      },
      "source": [
        "For the least square solution, we will take a matrix where $m>n$ (as explained in the Introduction), and compute the residual\n",
        "$$\n",
        "\\|Ax - b\\|\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDW1uiwp_6jC",
        "outputId": "3769b464-0bc6-4331-c6fb-079e89f4f463"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|Ax - b| = 0.87912444697435\n"
          ]
        }
      ],
      "source": [
        "A = np.array([[0, 2,4], [4, 1,2], [-2, 3,5], [-3,2,7]])\n",
        "b = np.array([ [4],[5],[6],[4] ]);\n",
        "x = leastsolve(A,b)\n",
        "print(\"|Ax - b| =\", np.linalg.norm(np.dot(A,x) - b ) )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "In this report we explored some algorithms for matrix factorization, which were later useful to solve systems of linear equations, as well as sparse matrix-vector multiplication. \n",
        "\n",
        "In general, the algorithms performed pretty well, even the Gram-Schmidt QR decomposition which should be one of the slowest, and the most \"inaccurate\" from floating point operation accumulation. However, from the tests that were done (which were not at a big scale), the algoritms did surprisingly well.\n",
        "\n",
        "There are a few things that need to be explored out of this report, that could be important implementations to some simulations. The first one is the implementation of sparse matrix-matrix multiplication, since this could help improve the simulation of time evolution in 1D Quantum systems. In one dimension, with a time independent Hamiltonian $\\hat{H}$, the evolution of a quantum state $\\psi(x,0)$ is governed by the unitary propagator\n",
        "\n",
        "$$\n",
        "\\psi(x,t) = e^{-i\\hat{H}t/\\hbar}\\psi(x,0)\n",
        "$$\n",
        "\n",
        "Where $\\hat{H}$ is the Hamiltonian, which from the correspondance principle can be written as\n",
        "\n",
        "$$\n",
        "-\\dfrac{\\hbar^2}{2m}\\dfrac{\\text{d}}{\\text{d}x^2} + V(x),\n",
        "$$\n",
        "\n",
        "In general, this is an infinite-dimensional matrix, but can be approximated with the differential matrix operator seen in the lectures, but with a really big matrix. The problem with this time-evolution, is that the matrix-exponential is an infinite-series that needs to multiply $\\hat{H}$ an incredibly amout of times, which is computationally very intensive. \n",
        "\n",
        "In matrix representation $\\hat{H}$ is usually a sparse matrix, hence I believe that this algorithm could be more efficient if we implement matrix exponentiation with sparse matrix-matrix multiplication.\n",
        "\n",
        "Another thing that can be improved in the implementations given in this report, is the least squares solution whenever there is an infinite amount of solutions, these cases were not implemented since I do not know how to deal with them"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "leogabac_Lab2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}