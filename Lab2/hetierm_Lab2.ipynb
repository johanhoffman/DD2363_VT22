{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT22/blob/main/template-report-lab-X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 2: Matrix Factorization**\n",
        "**Marc HÃ©tier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is distributed under a certain license. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Pdll1Xc9WP0e",
        "outputId": "9e782dc7-4ad0-48f9-d45c-2200e83dec4d"
      },
      "outputs": [],
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2020 Johan Hoffman (jhoffman@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "outputs": [],
      "source": [
        "# Load neccessary modules.\n",
        "from google.colab import files\n",
        "\n",
        "import time\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5zMzgPlRAF6"
      },
      "source": [
        "In this lab, we focus on matrix factorization through different problems, which involve the most basic operation in applied mathematic. First, we will implement a sparse format for matrices, and a sparse matrix-dense vector product (Problem 1). In Problem 2, 3 and 5, we interest on QR algorithm and it's use to solve linear problems or find eignevalues. On Problem 4, we implement a least square method to solve a linear problem, and finally in Problem 6 we look for a blocked matrix-matrix product.\n",
        "\n",
        "Test will also be implemented, at least to verify accuracy and sometimes to test the performances.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 1 : Sparse matrix-vector product\n",
        "We will first implement a class of sparse matrix, corresponding of CRS format. We add some methods to convert a numpy array into this format, and a print function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Sparse:\n",
        "    def __init__(self, size = (0,0), val = [], col=[], row=[]):\n",
        "        self.size = size\n",
        "        self.val = val\n",
        "        self.col = col\n",
        "        self.row_ptr = row\n",
        "\n",
        "    def __str__(self) -> None:\n",
        "        return \"Value   : \" + str(self.val) + '\\n' + \\\n",
        "                \"Col     : \" + str(self.col)  + '\\n' +  \\\n",
        "                \"Row_ptr : \" + str(self.row_ptr)\n",
        "    \n",
        "    def np_to_sp(self, A):\n",
        "        self.val, self.col, self.row_ptr = [], [], []\n",
        "        self.size = A.shape\n",
        "        N, M = A.shape\n",
        "        compt_row = 0\n",
        "        for ind_row in range(0, N):\n",
        "            test = 0\n",
        "            for ind_col in range(0, M):\n",
        "                value = A[ind_row, ind_col]\n",
        "                if value != 0:\n",
        "                    if not test:\n",
        "                        for _ in range(compt_row+1):\n",
        "                            self.row_ptr.append(len(self.val))\n",
        "                        test = 1\n",
        "                        compt_row = 0\n",
        "\n",
        "                    self.val.append(value)\n",
        "                    self.col.append(ind_col)\n",
        "    \n",
        "            if not test :\n",
        "                compt_row += 1\n",
        "        \n",
        "        for _ in range(compt_row+1):\n",
        "            self.row_ptr.append(len(self.val))\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For example :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A = np.array([[1,2],[0,3],[1,0]])\n",
        "A_s = Sparse()\n",
        "A_s.np_to_sp(A)\n",
        "print(\"A : \\n\", A)\n",
        "print(\"A_s :\\n\", A_s)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now implement the sparse-matrix vector product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sp_matrix_vector(A, b):\n",
        "    \"\"\"\n",
        "    Input : a sparse matrix A, a numpy array b\n",
        "    Output : numpy vector A*b\n",
        "    \"\"\"\n",
        "    assert len(A.size) == 2, \"A is not a matrix\"\n",
        "    assert len(b.shape) == 1, \"b is not a vector\"\n",
        "    assert b.shape[0] == A.size[1], \"dimension does not match\"\n",
        "\n",
        "    M = A.size[1]\n",
        "    rslt = np.zeros_like(b)\n",
        "    for i in range(M):\n",
        "        for j in range(A.row_ptr[i], A.row_ptr[i+1]):\n",
        "            rslt[i] += A.val[j] * b[A.col[j]]\n",
        "        \n",
        "    return rslt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 2 : QR factorization\n",
        "In this problem, we implement a QR factorization of a real quadratic matrix A, using Gram-Schmidt procedure.\n",
        "We first implement a subfunction which orthogonalize one vector versus a matrix, before using it on the main problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def modified_GS(Q, a):\n",
        "    \"\"\"\n",
        "    Input : an orthonormal matrix Q, a vector a\n",
        "    Output : result of Gram-Schmidt procedure applied to (Q, a). A vector q, orthonormal\n",
        "    with Q and a vector containing the coordinates of a in basis (Q, q)\n",
        "    \"\"\"\n",
        "    N, J = Q.shape\n",
        "    assert N==a.shape[0], \"dimension does not match\"\n",
        "    q = deepcopy(a)\n",
        "    coord = np.zeros(J+1)\n",
        "    for j in range(J):\n",
        "        ps = np.dot(Q[:,j], q)\n",
        "        q = q - ps*Q[:,j]\n",
        "        coord[j] = ps\n",
        "\n",
        "    norm = np.linalg.norm(q)\n",
        "    q = q/norm\n",
        "    coord[J] = norm\n",
        "    return q, coord"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def QR_decomp(A):\n",
        "    \"\"\"\n",
        "    Input : a real square matrix A\n",
        "    Output : a QR decomposition of A, using GS method\n",
        "    \"\"\"\n",
        "    N, M = A.shape\n",
        "    assert N==M, 'A is not a square matrix'\n",
        "    Q = np.zeros((N,N))\n",
        "    A_1 = A[:,0]\n",
        "    norm_A_1 = np.linalg.norm(A_1)\n",
        "    Q[:,0] = deepcopy(A_1)/norm_A_1\n",
        "    R = np.zeros((N,N))\n",
        "    R[0,0] = norm_A_1\n",
        "\n",
        "    for ind in range(1, N):\n",
        "        A_1 = A[:,ind]\n",
        "        q, coord = modified_GS(Q[:,0:ind], A_1)\n",
        "        Q[:,ind] = q\n",
        "        R[0:ind+1, ind] = coord\n",
        "\n",
        "    return Q, R"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 3 : Direct solver Ax = b\n",
        "\n",
        "In this problem, we have to solve a linear problem $Ax = b$ using a direct method. We will use the previous problem to solve this system by a clever way. Indeed, using the QR decomposition of $A = QR$, the system $Ax = b$ is equivalent at \n",
        "$$\\text{solve } Qy = b$$\n",
        "and then $$ \\text{solve } Rx = y$$\n",
        "\n",
        "Since $R$ is upper diagonal, solving the first problem is easy, and because $Q$ is orthonormal, solving the second problem is also easy.\n",
        "\n",
        "First, we implement a solver for $Rx = b$, $R$ being upper triangular."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solver_TS(R, b):\n",
        "    \"\"\"\n",
        "    Input : an invertible upper triangular matrix R, a vector b\n",
        "    Output : the vector x such that Rx = b\n",
        "    \"\"\"\n",
        "    N = len(R)\n",
        "    x = np.zeros(N)\n",
        "    for i in range(N-1, -1, -1):\n",
        "        tmp = b[i]\n",
        "        for j in range(i+1, N):\n",
        "            tmp -= x[j] * R[i,j]\n",
        "        x[i] = tmp / R[i,i]\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we implement the solver for the original problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def solver(A, b):\n",
        "    \"\"\"\n",
        "    Input : a square matrix A, invertible, a vector b\n",
        "    Output : the vector x such that Rx = b\n",
        "    \"\"\"\n",
        "    Q, R = QR_decomp(A)\n",
        "    y = np.transpose(Q) @ b\n",
        "    x = solver_TS(R, y)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 4 : Least squares problem\n",
        "In this problem, we implement a least squares method to solve $Ax=b$, with $A$ a rectangular matrix.\n",
        "\n",
        "The least squares problem can be formulate as follow :\n",
        "\n",
        "> Find $\\bar{x}$ such that $$\\forall y, \\Vert A\\bar{x} - b \\Vert \\leq \\Vert Ay - b\\Vert$$\n",
        "\n",
        "We know (see example 2.17 of the book) that such $\\bar{x}$ is given by :\n",
        "$$A^T A \\bar{x} = A^Tb$$\n",
        "where $A^TA$ is a square invertible matrix (as soon as $A$ has full rank). Then, we can use Problem 3 to solve those normal equations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def least_squares_pb(A, b):\n",
        "    \"\"\"\n",
        "    Input : A rectangular matrix A, which has full rank, a vector b\n",
        "    Output : minimizer x of ||A x - b ||\n",
        "    \"\"\"\n",
        "    A_t = np.transpose(A)\n",
        "    x = solver(A_t @ A, A_t @ b)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 5 : QR eigenvalue algorithm\n",
        "I propose here the basic version of the QR algorithm (no improvements using Hessenberg matrix or shift). This version is the slowest, but as efficient as the others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def QR_eigenpair(A, m):\n",
        "    \"\"\"\n",
        "    Input : a square matrix A, a number of iteration m\n",
        "    Output : m-th iteration of the QR algorithm\n",
        "    \"\"\"\n",
        "    U = np.eye(len(A))\n",
        "    T = deepcopy(A)\n",
        "    for _ in range(m):\n",
        "        Q, R = QR_decomp(T)\n",
        "        T = R @ Q\n",
        "        U = U @ Q\n",
        "\n",
        "    return T, U"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 6 : Blocked matrix-matrix product\n",
        "We implement a blocked matrix-matrix product, giving the possibility to the user to choose the block sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def blocked_matrix_matrix_product(A, B, blockA, blockB):\n",
        "    \"\"\"\n",
        "    Input : 2 matrices A and B, a list of block size for A and B,\n",
        "        under the form [[(nbr_row, nbr_col), (,),..], [(,), ...(,)]]\n",
        "    Output : result of A@B\n",
        "    \"\"\"\n",
        "    m, _ = A.shape\n",
        "    _, p = B.shape\n",
        "    C = np.zeros((m, p))\n",
        "\n",
        "    nbr_block_A = blockA.shape\n",
        "    nbr_block_B = blockB.shape\n",
        "\n",
        "    ind_row_C = 0 # to know in which row of C (and A) we are\n",
        "\n",
        "    for i in range(nbr_block_A[0]):\n",
        "        ind_col_C = 0 # to know in which col of C (and B) we are\n",
        "        for j in range(nbr_block_B[1]):\n",
        "            ind_row_B = 0\n",
        "            ind_col_A = 0\n",
        "            for k in range(nbr_block_A[1]):\n",
        "                ind_row_C_end = ind_row_C + blockA[i,k][0]\n",
        "                ind_col_C_end = ind_col_C + blockB[k,j][1]\n",
        "                ind_col_A_end = ind_col_A + blockA[i,k][1]\n",
        "                ind_row_B_end = ind_row_B + blockB[k,j][0]\n",
        "\n",
        "                block = A[ind_row_C:ind_row_C_end, ind_col_A:ind_col_A_end] @ B[ind_row_B:ind_row_B_end, ind_col_C:ind_col_C_end]                \n",
        "                C[ind_row_C:ind_row_C_end, ind_col_C:ind_col_C_end] += block\n",
        "\n",
        "                ind_col_A = ind_col_A_end\n",
        "                ind_row_B = ind_row_B_end\n",
        "            \n",
        "            ind_col_C = ind_col_C_end\n",
        "        ind_row_C = ind_row_C_end\n",
        "    return C"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 1 : test\n",
        "To test our programm, we create matrices of the form $$Tridiag(-1,2,-1) $$ and compare the computational time and the result between a dense matrix-vector multiplication, and a sparse matrix-vector multiplication for several sizes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_tridiag(size):\n",
        "    d = np.diag([2 for _ in range(size)], 0)\n",
        "    sd = [-1 for _ in range(size-1)]\n",
        "    ud = np.diag(sd, 1)\n",
        "    ld = np.diag(sd, 1)\n",
        "    return d + ud + ld\n",
        "\n",
        "\n",
        "sizes = [10, 100, 500, 1000]\n",
        "time_np = [0,0,0,0]\n",
        "time_sp = [0,0,0,0]\n",
        "accuracy = [0,0,0,0]\n",
        "\n",
        "for ind, size in enumerate(sizes):\n",
        "    vec = np.ones(size)\n",
        "    T = create_tridiag(size)\n",
        "    T_sp = Sparse()\n",
        "    T_sp.np_to_sp(T)\n",
        "\n",
        "    start_np = time.time()\n",
        "    rslt_np = T@vec\n",
        "    time_np[ind] = time.time() - start_np\n",
        "\n",
        "    start_sp = time.time()\n",
        "    rslt_sp = sp_matrix_vector(T_sp, vec)\n",
        "    time_np[ind] = time.time() - start_sp\n",
        "\n",
        "    accuracy[ind] = np.linalg.norm(rslt_sp - rslt_np)\n",
        "\n",
        "\n",
        "plt.figure(2)\n",
        "plt.plot(sizes, accuracy, '-x')\n",
        "plt.xlabel(\"Size of matrix\")\n",
        "plt.ylabel(\"Difference between the result of \\n dense and sparse multiplication\")\n",
        "\n",
        "plt.figure(1)\n",
        "plt.plot(sizes, time_np, 'x-')\n",
        "plt.plot(sizes, time_sp, 'x-')\n",
        "plt.legend([\"Time for dense matrix\", \"Time for sparse matrix\"])\n",
        "plt.xlabel(\"Size of matrix\")\n",
        "plt.ylabel(\"Time (s)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the sparse matrix-vector multiplication is well implemented, and that its computational time remains almost constant while the size is increasing, which is not the case for dense matrix. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 2 : test\n",
        "To test if the QR factorization is correct, we can check the properties of matrices $Q$ and $R$ ($Q$ must be orthonormal, $R$ upper triangular) and that $QR = A$. We will use Frobenius norm for those test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 10\n",
        "A = np.random.random((n,n))\n",
        "Q, R = QR_decomp(A)\n",
        "\n",
        "## Test if R is upper triangular ##\n",
        "sum = 0\n",
        "for i in range(1,n):\n",
        "    for j in range(0, i):\n",
        "        sum += abs(R[i,j])\n",
        "print(\"Sum of the absolute value of lower part of R : \", sum)\n",
        "\n",
        "## Test orthonormality of Q ##\n",
        "print(\"Frobenius norm of Q^T @ Q - I : \", np.linalg.norm(np.transpose(Q)@Q - np.eye(n), 'fro'))\n",
        "\n",
        "## Test decomposition of A = QR ##\n",
        "print(\"Frobenius norm of A - QR : \", np.linalg.norm(A - Q@R, 'fro'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All the tests return expected results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 3 : test\n",
        "We test our solver for linear quadratic problem $Ax = b$ by two different way. First we compute $$\\Vert Ax - b \\Vert $$ and secondly we compute $$\\Vert x - y \\Vert $$ where $y$ is a manufactured solution of the problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n = 10\n",
        "A = np.random.random((n,n))\n",
        "b = np.random.random(n)\n",
        "\n",
        "x = solver(A, b)\n",
        "y = np.linalg.solve(A, b)\n",
        "print(\"Norm of Ax - b : \", np.linalg.norm(A@x - b))\n",
        "print(\"Norm of x - y : \", np.linalg.norm(x-y))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The difference between $Ax$ and $b$, or between $x$ and $y$ almost reached the machine precision, so we can consider our solution as exact."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 4 : test\n",
        "To test our implementation of the least square problem, we compute the norm of the residual :$$ \\Vert r \\Vert = \\Vert Ax - b\\Vert $$\n",
        "and we compare it with others norm $$\\Vert Ay-b\\Vert$$ for random vectors $y$, in order to test the argmin property of $x$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "n, m = 20, 10\n",
        "A = np.random.rand(n,m)\n",
        "b = np.random.rand(n)\n",
        "\n",
        "x = least_squares_pb(A, b)\n",
        "r = np.linalg.norm(A@x - b)\n",
        "print(\"Norm of Ax - b : \", r)\n",
        "\n",
        "test = 1\n",
        "for _ in range(1000):\n",
        "    y = np.random.random(m)\n",
        "    if np.linalg.norm(A@y-b) < r:\n",
        "        test = 0\n",
        "        break\n",
        "\n",
        "if not test:\n",
        "    print(\"We have find a better approximation; the implemntation is incorrect\")\n",
        "    print(np.linalg.norm(A@y-b))\n",
        "    print(np.linalg.det(A@np.transpose(A)))\n",
        "else:\n",
        "    print(\"We did not find a better approximation.\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 5 : test\n",
        "We can test the accuracy of the eigenpairs given by the QR algorithm by computing $\\det(A-\\lambda_i I)$ (should be equal at zero) and the norm of $\\Vert A v_i - \\lambda_i v_i\\Vert$ (should be 0 too)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A = np.random.rand(10,10)\n",
        "T, U = QR_eigenpair(A, 200)\n",
        "\n",
        "dets = []\n",
        "norms = []\n",
        "for i in range(10):\n",
        "    dets.append(np.linalg.det(A-T[i,i]*np.eye(10)))\n",
        "    norms.append(np.linalg.norm(A@U[:,i]-T[i,i]*U[:,i]))\n",
        "\n",
        "print(\"Determinants : \", dets)\n",
        "print(\"Norms : \", norms)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that after 200 iterations, all the eigenvalues have been found with a precision of $10^{-2}$. The convergence towards the eigenvectors is slowest. As said above, this algorithm can be highly improved using shift and Hessenberg reductions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 6 : test\n",
        "We test our algorithm on a particular example, where both matrices A and B have been divided into 4 blocks (2 in row, 2 in columns)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "A = np.reshape(np.arange(0, 21), (7,3))\n",
        "B = np.reshape(np.arange(5,29), (3, 8))\n",
        "\n",
        "blockA = np.array([[(2,2), (2,1)], [(5,2), (5,1)]])\n",
        "blockB = np.array([[(2,5), (2,3)], [(1,5), (1,3)]])\n",
        "\n",
        "C = blocked_matrix_matrix_product(A, B, blockA, blockB)\n",
        "\n",
        "print(\"DIfference between normal and blocked multiplication : \", np.linalg.norm(C - A @ B))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The difference between our implementation and the normal multiplication is null, so we can be confident in our work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusion\n",
        "\n",
        "All the problems have been succesfully implemented and tested. \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "template-report-lab-X.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
