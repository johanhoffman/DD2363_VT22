{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/johanhoffman/DD2363_VT22/blob/main/template-report-lab-X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RgtXlfYO_i7"
      },
      "source": [
        "# **Lab 3: Iterative methods**\n",
        "**Marc HÃ©tier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x_J5FVuPzbm"
      },
      "source": [
        "# **Abstract**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmB2noTr1Oyo"
      },
      "source": [
        "A short statement on who is the author of the file, and if the code is distributed under a certain license. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "Pdll1Xc9WP0e",
        "outputId": "9e782dc7-4ad0-48f9-d45c-2200e83dec4d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'KTH Royal Institute of Technology, Stockholm, Sweden.'"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"This program is a template for lab reports in the course\"\"\"\n",
        "\"\"\"DD2363 Methods in Scientific Computing, \"\"\"\n",
        "\"\"\"KTH Royal Institute of Technology, Stockholm, Sweden.\"\"\"\n",
        "\n",
        "# Copyright (C) 2020 Johan Hoffman (jhoffman@kth.se)\n",
        "\n",
        "# This file is part of the course DD2365 Advanced Computation in Fluid Mechanics\n",
        "# KTH Royal Institute of Technology, Stockholm, Sweden\n",
        "#\n",
        "# This is free software: you can redistribute it and/or modify\n",
        "# it under the terms of the GNU Lesser General Public License as published by\n",
        "# the Free Software Foundation, either version 3 of the License, or\n",
        "# (at your option) any later version.\n",
        "\n",
        "# This template is maintained by Johan Hoffman\n",
        "# Please report problems to jhoffman@kth.se"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28xLGz8JX3Hh"
      },
      "source": [
        "# **Set up environment**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2PYNusD08Wa"
      },
      "source": [
        "To have access to the neccessary modules you have to run this cell. If you need additional modules, this is where you add them. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "Xw7VlErAX7NS"
      },
      "outputs": [],
      "source": [
        "# Load neccessary modules.\n",
        "# from google.colab import files\n",
        "import numpy as np\n",
        "from copy import deepcopy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnO3lhAigLev"
      },
      "source": [
        "# **Introduction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQvukXZq5U5"
      },
      "source": [
        "# **Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 1 : Jacobi iteration for $Ax= b$\n",
        "To solve the problem $Ax=b$ we often use preconditionner before applying fixed point iteration. It exists different types of preconditionner, and some of them are based on matrix splitting. It is the case for Jacobi iteration and Gauss Seidel iteration (cf Problem 2).\n",
        "For the general cases, we write :\n",
        "$$ A = A_1 + A_2 $$\n",
        " with $A_1$ invertible.\n",
        "Then, solution for $Ax = b$ is equivalently a solution of\n",
        "$$(I+A_1^{-1})x = A_1^{-1}b $$\n",
        "Note that we gain something only if solving both problem $A_1 y = b$ and $ (I+A_1^{-1})x = y$ are easier than the original one. \n",
        "\n",
        "The fixed point iteration for this problem writes :\n",
        "$$x^{k+1} = -A_1^{-1}A_2 x^{k} + A_1^{-1}b $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For **Jacobi iteration** we use $A_1 = Diag(A)$. This leads to a simple composant wise iteration :\n",
        "\n",
        "$$ \\forall i, \\;\\; x_i^{k+1} = \\dfrac{1}{a_{ii}} (b_i - \\sum_{j \\neq i}a_{ij}x_j^k) $$\n",
        "\n",
        "To ensure convergence criteria, we can assume that $A$ is striclty diagonally dominant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Jacobi_ite(A, b, x_0, m):\n",
        "    \"\"\"\n",
        "    Input : matrix A, diagonal dominant vector b, initial guess x_0 and number of iterations m\n",
        "    Output : m th iteration of Jacobi iteration for problem Ax = b \n",
        "    \"\"\"\n",
        "    n = len(A)\n",
        "    test = 1\n",
        "    for i in range(n):\n",
        "        summ = sum(abs(A[i,:]))\n",
        "        if 2*abs(A[i,i]) <= summ:\n",
        "            test = 0\n",
        "            break\n",
        "    if not test:\n",
        "        print(\"A is not strictly diagonally dominant; method can not converge\")\n",
        "\n",
        "    x = deepcopy(x_0)\n",
        "    cst_term = np.array([b[i]/A[i,i] for i in range(n)])\n",
        "\n",
        "    for _ in range(m):\n",
        "        scd_term = np.array([1/A[i,i] * sum([A[i,j]*x[j] for j in range(n) if j != i])\\\n",
        "                     for i in range(n)])\n",
        "        x = cst_term - scd_term\n",
        "    \n",
        "    return x   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 2 : Gauss-Seidel iteration for $Ax=b$\n",
        "\n",
        "As said before, this method also use splitting preconditionner but this time with $A_1$ equal at the lower triangular part of $A$. This leads to the following componant wise iteration :\n",
        "\n",
        "$$ \\forall i, \\;\\; x_i^{k+1} = \\dfrac{1}{a_{ii}} (b_i - \\sum_{j = 1}^{i-1}a_{ij}x_j^{k+1} - \\sum_{j = i+1}^{n}a_{ij}x_j^{k}) $$\n",
        "\n",
        "\n",
        "The method converges when $A$ is positive definite or strictly diagonally dominant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Gauss_Seidel_ite(A, b, x_0, m):\n",
        "    \"\"\"\n",
        "    Input : matrix A, diagonal dominant vector b, initial guess x_0 and number of iterations m\n",
        "    Output : m th iteration of Jacobi iteration for problem Ax = b \n",
        "    \"\"\"\n",
        "    n = len(A)\n",
        "    test = 1\n",
        "    for i in range(n):\n",
        "        summ = sum(abs(A[i,:]))\n",
        "        if 2*abs(A[i,i]) <= summ:\n",
        "            test = 0\n",
        "            break\n",
        "    if not test:\n",
        "        print(\"A is not strictly diagonally dominant; method can not converge\")\n",
        "\n",
        "    x = deepcopy(x_0)\n",
        "    cst_term = np.array([b[i]/A[i,i] for i in range(n)])\n",
        "    for _ in range(m):\n",
        "        x_1 = np.zeros(n)\n",
        "        for i in range(n):\n",
        "            scd_term = sum([A[i,j]*x_1[j] for j in range(i)])/A[i,i]\n",
        "            trd_term = sum([A[i,j]*x[j] for j in range(i+1, n)])/A[i,i]\n",
        "            x_1[i] = cst_term[i] - scd_term - trd_term\n",
        "        x = x_1\n",
        "    \n",
        "    return x   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 3 : Newton's method for scalar non linear function\n",
        "\n",
        "In this problem, we use Newton's method to solve a scalar non linear equation $$f(x) = 0$$ where $f$ is a $C^1$ function over some interval $I$, with $f'(x) \\neq 0$ for $x\\in I$.\n",
        "\n",
        "This method is an iterative one, based on the following sequence :\n",
        "$$ x^{k+1} = x^k - \\dfrac{f(x^k)}{f'(x^k)} $$\n",
        "\n",
        "Geometrically, this means that $x^{k+1}$ corresponds to the zero value of the tangent line at point $x^k$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Newton_method(f, df, x, tol, ite = 500):\n",
        "    \"\"\"\n",
        "    Input : a function f, its derivative df, an initial guess x\n",
        "            and a tolerance criteria tol\n",
        "    Output : the convergence point if the tolerance is reached, or\n",
        "            point after ite=500 iterations\n",
        "    \"\"\"\n",
        "    xk = x\n",
        "    test = 0\n",
        "    for _ in range(ite):\n",
        "        f_xk = f(xk)\n",
        "        if np.abs(f_xk) < tol:\n",
        "            test = 1\n",
        "            break\n",
        "\n",
        "        df_xk = df(xk)\n",
        "        if df_xk == 0:\n",
        "            print(\"The derivative evaluated at xk = \", xk, \"is null\")\n",
        "            return None\n",
        "\n",
        "        xk = xk - f_xk/df_xk\n",
        "    \n",
        "    if np.abs(f_xk) < tol:\n",
        "            test = 1\n",
        "\n",
        "    if test:\n",
        "        return xk\n",
        "    else:\n",
        "        print(\"Stopping criteria not reached, end after \", ite, \"iterations\")\n",
        "        return xk      "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem 4 : GMRES method for Ax = b\n",
        "\n",
        "GMRES (Generalized minimal residual) is a Krylov method for solving large problem $Ax = b$. It uses the following Arnoldi factorization, obtained for exemple using Gram-Schmidt procedure on the vectorial space $K_m(A, b) = span(b, Ab, \\ldots, A^{m-1}b)$ :\n",
        "$$ A Q_m = Q_{m+1} H_m$$\n",
        "with $Q_m$ othonormal matrix of size $n\\times m$, $H_m$ Hessenberg matrix.\n",
        "\n",
        "The GMRES method consits to minimizing the 2-norm of the residual over $K_m(A,b)$ *ie* :\n",
        "$$x_m = \\underset{x \\in K_m(A,b)}{\\text{argmin}} \\Vert Ax -b \\Vert $$\n",
        "\n",
        "Using the fact that $K_m(A,b) = span(Q_m)$ and that $b = \\Vert b \\Vert Q_{m+1} e_1$, we have :\n",
        "\n",
        "\\begin{align*}\n",
        "x_m &= Q_m \\times \\underset{x \\in \\mathbb{R}^m}{\\text{argmin}} \\Vert AQ_mx - \\Vert b \\Vert Q_{m+1} e_1 \\Vert \\\\\n",
        "x_m &= Q_m \\times \\underset{x \\in \\mathbb{R}^m}{\\text{argmin}} \\Vert Q_{m+1}(H_m x - \\Vert b \\Vert e_1) \\Vert \\\\\n",
        "x_m &= Q_m \\times \\underset{x \\in \\mathbb{R}^m}{\\text{argmin}} \\Vert H_m x - \\Vert b \\Vert e_1 \\Vert\n",
        "\\end{align*}\n",
        "\n",
        "This last problem is only of size $(m+1) \\times m$ with $m << n$, while the initial one was of size $n\\times n$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {},
      "outputs": [],
      "source": [
        "def Gram_schmidt(Q, a):\n",
        "    \"\"\"\n",
        "    Input : an orthonormal matrix Q, a vector x\n",
        "    Output : result of Gram-Schmidt procedure applied to (Q, a).\n",
        "    \"\"\"\n",
        "    _, M = Q.shape\n",
        "    q = deepcopy(a)\n",
        "    coord = np.zeros(M)\n",
        "    for m in range(M):\n",
        "        ps = np.dot(Q[:,m], q)\n",
        "        q = q - ps*Q[:,m]\n",
        "        coord[m] = ps\n",
        "    \n",
        "    norm = np.linalg.norm(q)\n",
        "    return q, coord, norm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {},
      "outputs": [],
      "source": [
        "def GMRES(A, b, m):\n",
        "    \"\"\"\n",
        "    Input : matrix A, vector b, range of approximation m\n",
        "    Output : result of GMRES applied to Ax = b, approximation at level m\n",
        "    \"\"\"\n",
        "    n = len(A)\n",
        "\n",
        "    ## Compute the Arnoldi factorization\n",
        "    norm_b = np.linalg.norm(b)\n",
        "    a = b/norm_b\n",
        "\n",
        "    Q_m = np.zeros((n, m+1))\n",
        "    Q_m[:,0] = a\n",
        "    H_m = np.zeros((m+1, m))\n",
        "\n",
        "    for ind in range(0,m):\n",
        "        a = A@Q_m[:,ind]\n",
        "        q, coord, norm = Gram_schmidt(Q_m[:,0:ind+1], a)\n",
        "        Q_m[:,ind+1] = q/norm\n",
        "        H_m[0:ind+1, ind] = coord\n",
        "        H_m[ind+1, ind] = norm\n",
        "    \n",
        "    ## Solve reduce problem\n",
        "    e_1 = np.zeros((m+1, 1))\n",
        "    e_1[0] = norm_b\n",
        "    x = np.linalg.lstsq(H_m, e_1, rcond=None)[0]\n",
        "    x = Q_m[:,0:m]@x\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsQLT38gVbn_"
      },
      "source": [
        "# **Results**\n",
        "\n",
        "## Test for problem 1 and 2\n",
        "We will test the Jacobi and Gauss-Seidel methods using the same problem. We first construct a matrix $A$, striclty diagonally dominant, and then copute the residual and the difference with an exact solution, given by the built in function of numpy.linalg."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Residual norm for Jacobi method, after 10 iterations :  0.44120422056490405 difference with the exact sol :  0.000636541423002796\n",
            "Residual norm for Jacobi method, after 50 iterations :  0.000125925978590891 difference with the exact sol :  1.8167720306517036e-07\n",
            "\n",
            " Residual norm for Gauss-Seidel method, after 5 iterations :  0.00013346445496273884 difference with the exact sol :  8.146161408684108e-07\n",
            "Residual norm for Gauss-Seidel method, after 10 iterations :  3.360420361660094e-08 difference with the exact sol :  1.4429833711263496e-10\n"
          ]
        }
      ],
      "source": [
        "A = np.reshape(np.arange(10, 135, 5), (5,5))\n",
        "for i in range(5):\n",
        "    A[i,i] = sum(A[i,j] for j in range(5))\n",
        "b = np.ones(5)\n",
        "\n",
        "x = np.linalg.solve(A, b)\n",
        "\n",
        "x_J = Jacobi_ite(A, b, np.zeros(5), 10)\n",
        "res_J = np.linalg.norm(A@x_J - b)\n",
        "diff_J = np.linalg.norm(x_J-x)\n",
        "print(\"Residual norm for Jacobi method, after 10 iterations : \", res_J, \"difference with the exact sol : \", diff_J)\n",
        "x_J = Jacobi_ite(A, b, np.zeros(5), 50)\n",
        "res_J = np.linalg.norm(A@x_J - b)\n",
        "diff_J = np.linalg.norm(x_J-x)\n",
        "print(\"Residual norm for Jacobi method, after 50 iterations : \", res_J, \"difference with the exact sol : \", diff_J)\n",
        "\n",
        "x_GS = Gauss_Seidel_ite(A, b, np.zeros(5), 5)\n",
        "res_GS = np.linalg.norm(A@x_GS - b)\n",
        "diff_GS = np.linalg.norm(x_GS-x)\n",
        "print(\"\\n Residual norm for Gauss-Seidel method, after 5 iterations : \", res_GS, \"difference with the exact sol : \", diff_GS)\n",
        "x_GS = Gauss_Seidel_ite(A, b, np.zeros(5), 10)\n",
        "res_GS = np.linalg.norm(A@x_GS - b)\n",
        "diff_GS = np.linalg.norm(x_GS-x)\n",
        "print(\"Residual norm for Gauss-Seidel method, after 10 iterations : \", res_GS, \"difference with the exact sol : \", diff_GS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Both method converges in few iterations, but Gauss-Seidel method is much faster than Jacobi method (ten times less iteration are required to obtain the same residual norm)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test for Problem 3 :\n",
        "\n",
        "We will test the Newton's method on the function $f(x) = x^2 -1$, using a starting point at $x=0.5$, and at $x=-0.5$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting point :  0.5\n",
            "Residual convergence :  9.292229696811205e-08\n",
            "Difference with expected root 1 :  4.6461147373833e-08\n",
            "\n",
            "Starting point :  -0.5\n",
            "Residual convergence :  9.292229696811205e-08\n",
            "Difference with expected root -1 :  4.6461147373833e-08\n"
          ]
        }
      ],
      "source": [
        "f = lambda x:x**2-1\n",
        "df = lambda x:2*x\n",
        "\n",
        "x_1 = 0.5\n",
        "x_2 = -0.5\n",
        "\n",
        "x_f1 = Newton_method(f, df, x_1, 10e-6)\n",
        "\n",
        "print(\"Starting point : \", 0.5)\n",
        "print(\"Residual convergence : \", np.abs(f(x_f1)))\n",
        "print(\"Difference with expected root 1 : \", np.abs(x_f1-1))\n",
        "\n",
        "x_f2 = Newton_method(f, df, x_2, 10e-6)\n",
        "\n",
        "print(\"\\nStarting point : \", -0.5)\n",
        "print(\"Residual convergence : \", np.abs(f(x_f2)))\n",
        "print(\"Difference with expected root -1 : \", np.abs(x_f2+1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The Newton method converge with a quadratic rate since the function $f$ is $C^2$. Moreover, we can see the perfect symmetry between the residual/difference of the two starting points. This was expected because of the problem's symmetry."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test for problem 4\n",
        "\n",
        "Finally, we can test the GMRES function using a medium size matrix $A$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Residual after 10 iterations : 0.002251400858594942\n",
            "Residual after 25 iterations : 2.4161676951621126e-09\n",
            "Residual after 50 iterations : 6.388347155134135e-13\n"
          ]
        }
      ],
      "source": [
        "def create_tridiag(size):\n",
        "    d = np.diag([5 for _ in range(size)], 0)\n",
        "    sd = [-1 for _ in range(size-1)]\n",
        "    ud = np.diag(sd, 1)\n",
        "    ld = np.diag(sd, 1)\n",
        "    return d + ud + ld\n",
        "\n",
        "A = create_tridiag(200)\n",
        "b = np.ones(200)\n",
        "\n",
        "x = GMRES(A, b, 10)\n",
        "print(\"Residual after 10 iterations :\", np.linalg.norm(A@x - b))\n",
        "\n",
        "x = GMRES(A, b, 25)\n",
        "print(\"Residual after 25 iterations :\", np.linalg.norm(A@x - b))\n",
        "\n",
        "x = GMRES(A, b, 50)\n",
        "print(\"Residual after 50 iterations :\", np.linalg.norm(A@x - b))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that after 50 iterations, the method gives a solution which almost reach the machine precision. Then GMRES seems to be well implemented.\n",
        "Note that I have change the diagonal of A from 2 to 5 to get better convergence rate (which is highly connected with the eigenvalue of A)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4GLBv0zWr7m"
      },
      "source": [
        "# **Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcsDSoRXHZe"
      },
      "source": [
        "All the methods implemented wok well, and it is interesting to see their different convergence rate. Then, depending on $A$, one should privilige one method in particular, instead of the others."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "template-report-lab-X.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
